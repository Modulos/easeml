#!/usr/bin/python

import argparse
import json
import numpy as np
import os

from sklearn.externals import joblib
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.preprocessing import StandardScaler

import easemlschema.schema as sch
import easemlschema.dataset as ds

dir_path = os.path.dirname(os.path.realpath(__file__))

with open(os.path.join(dir_path, "schema-in.json")) as f:
    schemaIn = json.load(f)

with open(os.path.join(dir_path, "schema-out.json")) as f:
    schemaOut = json.load(f)

schIn = sch.Schema.load(schemaIn)
schOut = sch.Schema.load(schemaOut)

className = "cls"

#schIn = sch.Schema.load(schema["input"])
#schOut = sch.Schema.load(schema["output"])

if __name__ == "__main__":

    description = "K-NN Classifier."
    parser = argparse.ArgumentParser(description=description)

    parser.add_argument("--data", required=True, help="directory containing input data")
    parser.add_argument("--config", required=True, help="config file")
    parser.add_argument("--output", required=True, help="directory where the memory will be dumped")

    args = parser.parse_args()

    datasetIn = ds.Dataset.load(os.path.join(args.data, "input"))
    datasetOut = ds.Dataset.load(os.path.join(args.data, "output"))

    # Infer schemas.
    srcSchemaIn = datasetIn.infer_schema()
    srcSchemaOut = datasetOut.infer_schema()

    matchSchemaIn = schIn.match(srcSchemaIn, build_matching=True)
    matchSchemaOut = schOut.match(srcSchemaOut, build_matching=True)

    inName = matchSchemaIn.nodes["s1"].src_name
    outname = matchSchemaOut.nodes["s1"].src_name

    outClassName = matchSchemaOut.category_classes[className].src_name
    outClassCategoriesList = datasetOut.children[outClassName].categories
    outClassCategories = dict([(outClassCategoriesList[i], i) for i in range(len(outClassCategoriesList))])

    X_vectors = []
    y_values = []
    for name in datasetIn.children:
        if isinstance(datasetIn.children[name], ds.Directory) and name in datasetOut.children:
            inValue = datasetIn.children[name].children[inName].data
            outValue = datasetOut.children[name].children[outname].categories[0]
            outIntValue = outClassCategories[outValue]

            X_vectors.append(inValue)
            y_values.append(outIntValue)

    X = np.stack(X_vectors)
    y = np.stack(y_values)

    # Load the config file.
    with open(args.config) as f:
        config = json.load(f)

    n_restarts_optimizer = config["n_restarts_optimizer"]
    max_iter_predict = config["max_iter_predict"]

    # Build and fit scaler.
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Initialize model and fit.
    model = GaussianProcessClassifier(n_restarts_optimizer=n_restarts_optimizer, max_iter_predict=max_iter_predict)
    model.fit(X, y)

    # Save model, scaler and category names for convenience.
    joblib.dump(scaler, os.path.join(args.output, "scaler.bin"))
    joblib.dump(model, os.path.join(args.output, "model.bin"))
    with open(os.path.join(args.output, "classes.json"), "w") as fp:
        json.dump(outClassCategoriesList, fp)
